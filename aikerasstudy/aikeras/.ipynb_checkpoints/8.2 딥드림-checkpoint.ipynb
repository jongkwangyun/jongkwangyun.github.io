{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff7870f",
   "metadata": {},
   "source": [
    "# 8.2 딥드림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab4fc4",
   "metadata": {},
   "source": [
    "딥드림(DeepDream)은 합성곱 신경망이 학습한 표현을 사용하여 예술적으로 이미지를 조작하는 기법입니다.\n",
    "\n",
    "### 8.2.1 케라스 딥드림 구현\n",
    "\n",
    "ImageNet에서 훈련한 컨브넷을 가지고 시작하겠습니다. 케라스에는 이렇게 사용할 수 있는 컨브넷이 많습니다. VGG16, VGG19, Xception, ResNet50 등입니다.\n",
    "\n",
    "#### 8-8 사전 훈련된 인셉션 V3 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11538c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)  # 모델을 훈련하지 않습니다. 이 명령은 모든 훈련 연산을 비활성화합니다.\n",
    "\n",
    "model = inception_v3.InceptionV3(weights='imagenet',\n",
    "                                include_top=False)  # 합성곱 기반 층만 사용한 인셉션 V3 네트워크를 만듭니다. 사전 훈련된 ImageNet 가중치와 함께 모델을 로드합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bd29d2",
   "metadata": {},
   "source": [
    "#### 코드 8-9 딥드림 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ce9c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_contributions = {  # 층 이름과 계수를 매핑한 딕셔너리입니다. 최대화하려는 손실에 층의 활성화가 기여할 양을 정합니다. 층 이름은 내장된 인셉션 V3 애플리케이션에 하드코딩되어 있는 것입니다. model.summary()를 사용하면 모든 층 이름을 확인할 수 있습니다.\n",
    "    'mixed2': 0.2,\n",
    "    'mixed3': 3.,\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87013c",
   "metadata": {},
   "source": [
    "#### 코드 8-10 최대화할 손실 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffd2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])  # 층 이름과 층 객체를 매핑한 딕셔너리를 만듭니다.\n",
    "loss = K.variable(0.)  # 손실을 정의하고 각 층의 기여 분을 이 스칼라 변수에 추가할 것입니다.\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output  # 층의 출력을 얻습니다.\n",
    "    \n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling  # 층 특성의 L2 노름 제곱을 손실에 추가합니다. 이미지 테두리는 제외하고 손실에 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b170c",
   "metadata": {},
   "source": [
    "#### 코드 8-11 경사 상습법 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c564b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dream = model.input  # 이 텐서는 생성된 딥드림 이미지를 저장합니다.\n",
    "\n",
    "grads = K.gradients(loss, dream)[0]  # 손실에 대한 딥드림 이미지의 그래디언트를 계산합니다.\n",
    "\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)  # 그래디언트를 정규화합니다(이 기교가 중요합니다).\n",
    "\n",
    "# 주어진 입력 이미지에서 손실과 그래디언트 값을 계산할 케라스 Function 객체를 만듭니다.\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "# 이 함수는 경사 상승법을 여러 번 반복하여 수행합니다.\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('...', i, '번째 손실 :', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9fa98a",
   "metadata": {},
   "source": [
    "#### 코드 8-12 연속적인 스케일에 걸쳐 경사 상승법 실행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe2a68b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/original_photo_deep_dream.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c7ad68d6cf6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mbase_image_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./datasets/original_photo_deep_dream.jpg'\u001b[0m  \u001b[1;31m# 사용할 이미지 경로를 씁니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_image_path\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 기본 이미지를 넘파이 배열로 로드합니다(이 함수는 코드 8-13에 정의되어 있습니다).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0moriginal_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-83e80961c650>\u001b[0m in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 사진을 열고 크기를 줄이고 인셉션 V3가 인식하는 텐서 포맷으로 변환하는 유틸리티 함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, target_size, interpolation)\u001b[0m\n\u001b[0;32m    360\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    361\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'L'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2974\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2975\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/original_photo_deep_dream.jpg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 하이퍼파라미터를 바꾸면 새로운 효과가 만들어집니다.\n",
    "step = 0.01  # 경사 상습법 단계 크기\n",
    "num_octave = 3  # 경사 상승법을 실행할 스케일 단계 횟수\n",
    "octave_scale = 1.4  # 스케일 간의 크기 비율\n",
    "\n",
    "iterations = 20  # 스케일 단계마다 수행할 경사 상승법 횟수\n",
    "\n",
    "max_loss = 10.  # 손실이 10보다 커지면 이상한 그림이 되는 것을 피하기 위해 경사 상승법 과정을 중지합니다.\n",
    "\n",
    "base_image_path = './datasets/original_photo_deep_dream.jpg'  # 사용할 이미지 경로를 씁니다.\n",
    "\n",
    "img = preprocess_image(base_image_path)  # 기본 이미지를 넘파이 배열로 로드합니다(이 함수는 코드 8-13에 정의되어 있습니다).\n",
    "\n",
    "original_shape = img.shape[1:3]\n",
    "# 경사 상승법을 실행할 스케일 크기를 정의한 튜플의 리스트를 준비합니다.\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i))\n",
    "                  for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "    \n",
    "successive_shapes = successive_shapes[::-1]  # 이 리스트를 크기 순으로 뒤집습니다.\n",
    "\n",
    "# 이미지의 넘파이 배열을 가장 작은 스케일로 변경합니다.\n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
    "\n",
    "for shape in successive_shapes:\n",
    "    print('처리할 이미지 크기', shape)\n",
    "    img = resize_img(img,shape)  # 딥드림 이미지의 스케일을 키웁니다.\n",
    "    img = gradient_ascent(img,\n",
    "                          # 경사 상승법을 실행하고 이미지를 변경합니다.\n",
    "                         iterations=iterations,\n",
    "                         step=step,\n",
    "                         max_loss=max_loss)\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)  # 작게 줄인 원본 이미지의 스케일을 높입니다. 픽셀 경계가 보일 것입니다.\n",
    "    same_size_original = resize_img(original_img, shape)  # 이 크기에 해당하는 원본 이미지의 고해상도 버전을 계산합니다.\n",
    "    lost_detail = sample_size_original - upscaled_shrunk_original_img  # 이 두 이미지의 차이가 스케일을 높였을 때 손실된 디테일입니다.\n",
    "    img += lost_detail  # 손실된 디테일을 딥드림 이미지에 다시 주입니다.\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png')\n",
    "    \n",
    "save_img(img, fname='./datasets/final_dream.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84391144",
   "metadata": {},
   "source": [
    "코드 8-13 유틸리티 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdde545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "              float(size[0]) / img.shape[1],\n",
    "              float(size[1]) / img.shape[2],\n",
    "              1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    image.save_img(fname, pil_img)\n",
    "    \n",
    "def preprocess_image(image_path):  # 사진을 열고 크기를 줄이고 인셉션 V3가 인식하는 텐서 포맷으로 변환하는 유틸리티 함수\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):  # 넘파이 배열을 적절한 이미지 포맷으로 변환하는 유틸리티 함수\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3))\n",
    "    # inception_v3.preprocess_input 함수에서 수행한 전처리 과정을 복원합니다.\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    \n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d56ba8",
   "metadata": {},
   "source": [
    "### 8.2.2 정리\n",
    "- 딥드림은 네트워크가 학습한 표현을 기반으로 컨브넷을 거꾸로 실행하여 입력 이미지를 생성합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
