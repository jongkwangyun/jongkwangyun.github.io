{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0d3ac0",
   "metadata": {},
   "source": [
    "시퀀스 데이터를 처리하는 기본적인 딥러닝 모델은 순환 신경망(recurrent neural network)과 1D 컨브넷(1D convnet) 두 가지입니다. 1D 컨브넷은 이전 장에서 다룬 2D 컨브넷의 1차원 버전입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068fd49",
   "metadata": {},
   "source": [
    "# 6.1 텍스트 데이터 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c8a6eb",
   "metadata": {},
   "source": [
    "컴퓨터 비전이 픽셀에 적용한 패턴 인식(pattern recoginition)인 것 처럼 자연어 처리(natural language processing)를 위한 딥러닝은 단어, 문장, 문단에 적용한 패턴 인식입니다.\n",
    "\n",
    "다른 모든 신경망과 마찬가지로 텍스트 원본을 입력으로 사용하지 못합니다. 딥러닝 모델은 수치형 텐서만 다룰 수 있습니다. 텍스트를 수치형 텐서로 변환하는 과정을 텍스트 벡터화(vectorizing test)라고 합니다. 여기에는 여러가지 방법이 있습니다.\n",
    "- 텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환합니다.\n",
    "- 텍스트를 문자로 나누가 각 문자를 하나의 벡터로 변환합니다.\n",
    "- 텍스트에서 단어나 문자의 n-그램(n-gram)을 추출하여 각 n-그램을 하나의 벡터로 변환합니다. n-그램은 연속된 단어나 문자의 그룹으로 텍스트에서 단어나 문자를 하나씩 이동하면서 추출합니다.\n",
    "\n",
    "텍스트를 나누는 이런 단위(단어, 문자, n-그램)를 토큰(token)이라고 힙니다. 그리고 텍스트를 토큰으로 나누는 작업을 토큰화(tokenization)라고 합니다. 모든 텍스트 벡터화 과정은 어떤 종류의 토큰화를 적용하고 생성된 토큰에 수치형 벡터를 연결하는 것으로 이루어집니다. 토큰과 벡터를 연결하는 방법은 여러 가지가 있습니다. 이 절에서 두 가지 주요 방법을 소개하겠습니다. 토큰의 원-핫 인코딩(one-hot encoding)과 토큰 임베딩(token embedding)(일반적으로 단어에 대해서만 사용되므로 단어 임베딩(word embedding)이라고도 부릅니다.\n",
    "\n",
    ">**n-그램과 BoW**\n",
    ">\n",
    ">단어 n-그램은 문장에서 추출한 N개(또는 그 이하)의 연속된 단어 그룹입니다. 같은 개념이 단어 대신 문자에도 적용 될 수 있습니다.\n",
    ">\n",
    ">다음은 간단한 예입니다. \"The cat sat on the mat.\"이란 문장을 생각해 보죠. 이 문장은 다음 2-그램의 집합으로 분해할 수 있습니다.\n",
    ">\n",
    ">{\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\", \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    ">또 다음 3-그램의 집합으로 분해할 수 있습니다.\n",
    ">\n",
    ">{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\", \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\", \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    ">\n",
    ">이런 집합을 각각 2-그램 가방(bag of 2-gram) 또는 3-그램 가방(bag of 3-gram)이라고 합니다.\n",
    "\n",
    "### 6.1.1 단어와 문자의 원-핫 인코딩\n",
    "\n",
    "원-핫 인코딩은 토큰을 벡터로 변환하는 가장 일반적이고 기본적인 방법입니다.\n",
    "\n",
    "#### 코드 6-1 단어 수준의 원-핫 인코딩하기(간단한 예)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e8baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']  # 초기 데이터: 각 원소가 샘플입니다. (이 예에서 하나의 샘플이 하나의 문장입니다. 하지만 문서 전체가 될 수도 있습니다.)\n",
    "\n",
    "token_index = {}  # 데이터에 있는 모든 토큰의 인덱스를 구축합니다.\n",
    "for sample in samples:\n",
    "    for word in sample.split():  # split() 메서드를 사용하여 샘플을 토큰으로 나눕니다. 실전에서는 구두점과 특수 문자도 사용합니다.\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1  # 단어마다 고유한 인덱스를 할당합니다. 인덱스 0은 사용하지 않습니다.\n",
    "            \n",
    "max_length = 10  # 샘플을 벡터로 변환합니다. 각 샘플에서 max_length까지 단어만 사용합니다.\n",
    "\n",
    "results = np.zeros(shape=(len(samples),\n",
    "                         max_length,\n",
    "                         max(token_index.values()) + 1))  # 결과를 저장할 배열입니다.\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c195fa0",
   "metadata": {},
   "source": [
    "#### 코드 6-2 문자 수준 원-핫 인코딩하기(간단한 예)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfbce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "characters = string.printable  # 출력 가능한 모든 아스키(ASCII) 문자\n",
    "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303dfa3",
   "metadata": {},
   "source": [
    "케라스에는 원본 텍스트 데이터를 단어 또는 문자 수준의 원-핫 인코딩으로 변환해주는 유틸리티가 있습니다.\n",
    "\n",
    "#### 코드 6-3 케라스를 사용한 단어 수준의 원-핫 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a8aff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9개의 고유한 토큰을 찾았습니다.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)  # 가장 빈도가 높은 1000개의 단어만 선택하도록 Tokenizer 객체를 만듭니다.\n",
    "tokenizer.fit_on_texts(samples)  # 단어 인덱스를 구축합니다.\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)  # 문자열을 정수 인덱스의 리스트로 변환합니다.\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')  # 직접 원-핫 이진 벡터 표현을 얻을 수 있습니다. 원-핫 인코딩 외에 다른 벡터화 방법들도 제공합니다.\n",
    "word_index = tokenizer.word_index  # 계산된 단어 인덱스를 구합니다.\n",
    "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151f4c1",
   "metadata": {},
   "source": [
    "원-핫 인코딩의 변종 중 하나는 원-핫 해싱(one-hot hashing) 기법입니다. 이 방식은 어휘 산전에 있는 고유한 토큰의 수가 너무 커서 모두 다루기 어려울 때 사용합니다. 한 가지 단점은 해시 충돌(hash collision)입니다. 2개의 단어가 같은 해시를 만들면 이를 바라보는 머신 러닝 모델은 단어 사이의 차이를 인식하지 못합니다. 해싱 공간의 차원이 해싱될 고유 토큰의 전체 개수보다 훨씬 크면 해시 충돌의 가능성은 감소합니다.\n",
    "\n",
    "#### 코드 6-4 해싱 기법을 사용한 단어 수준의 원-핫 인코딩하기(간단한 예)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43c8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "dimensionality = 1000  # 단어 크기가 1000인 벡터로 저장합니다. 1000개(또는 그 이상)의 단어가 있다면 해싱 충돌이 늘어나고 인코딩의 정확도가 감소될 것입니다.\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros((len(samples), max_length, dimensionality))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality  # 단어를 해싱하여 0과 1000 사이의 랜덤한 정수 인덱스로 변환합니다.\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62341bc",
   "metadata": {},
   "source": [
    "### 6.1.2 단어 임베딩 사용하기\n",
    "\n",
    "단어와 벡터를 연관짓는 강력하고 인기 있는 또 다른 방법은 단어 임베딩이라는 밀집 단어 벡터(word vector)를 사용하는 것입니다.\n",
    "\n",
    "단어 임베딩을 만드는 방법은 두 가지입니다.\n",
    "- (문서 분류나 감성 예측 같은) 관심 대상인 문제와 함께 단어 임베딩을 학습합니다. 이런 경우에는 랜덤한 단어 벡터로 시작해서 신경망의 가중치를 학습하는 것과 같은 방식으로 단어 벡터를 학습합니다.\n",
    "- 풀려는 문제가 아니고 다른 머신 러닝 작업에서 미리 계산된 단어 임베딩을 로드합니다. 이를 사전 훈련된 단어 임베딩(pretrained word embedding)이라고 합니다.\n",
    "\n",
    "#### Embedding 층을 사용하여 단어 임베딩 학습하기\n",
    "단어와 밀집 벡터를 연관짓는 가장 간단한 방법은 랜덤하게 벡터를 선택하는 것입니다. 이 방식의 문제점은 임베딩 공간이 구조적이지 않다는 것입니다. 예를 들어 accurate와 exact 단어는 대부분 문장에서 비슷한 의미로 사용되지만 완전히 다른 임베딩을 가집니다.\n",
    "\n",
    "단어 벡터 사이에 좀 더 추상적이고 기하학적인 관계를 얻으려면 단어 사이에 있는 의미 관계를 반영해야 합니다. 단어 임베딩은 언어를 기하학적 공간에 매핑하는 것입니다. 예를 들어 잘 구축된 임베딩 공간에서는 동의어가 비슷한 단어 벡터로 임베딩될 것입니다. 일반적으로 두 단어 벡터 사이의 거리(L2 거리)는 이 단어 사이의 의미 거리와 관계되어 있습니다(멀리 떨어진 위치에 임베딩된 단어 의미는 서로 다른 반면에 비슷한 단어들은 가까이 임베딩됩니다).\n",
    "\n",
    "새로운 작업에는 새로운 임베딩을 학습하는 것이 타당합니다.\n",
    "\n",
    "#### 코드 6-5 Embedding 층의 객체 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b5ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64)  # Embedding 층은 적어도 2개의 매개변수를 받습니다. 가능한 토큰의 개수(여기서는 1000으로 단어 인덱스 최댓값+1입니다)와 임베딩 차원(여기서는 64)입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e2fe2",
   "metadata": {},
   "source": [
    "Embedding 층을 (특정 단어를 나타내는) 정수 인덱스를 밀집 벡터로 매핑하는 딕셔너리로 이해하는 것이 가장 좋습니다.\n",
    "\n",
    "```단어 인덱스 → Embedding 층 → 연관된 단어 벡터```\n",
    "\n",
    "#### 코드 6-6 Embedding 층에 사용할 IMDB 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab3cb622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\keras\\datasets\\imdb.py:99: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "d:\\git\\jongkwangyun.github.io\\aikerasstudy\\aikeras\\lib\\site-packages\\keras\\datasets\\imdb.py:100: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "# numpy 업데이트로 인한 추가\n",
    "import numpy as np\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "max_features = 10000  # 특성으로 사용할 단어의 수\n",
    "maxlen = 20  # 사용할 텍스트의 길이(가장 빈번한 max_features개의 단어만 사용합니다)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)  # 정수 리스트로 데이터를 로드합니다.\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)  # 리스트를 (samples, max_len) 크기의 2D 정수 텐서로 변환합니다.\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60494928",
   "metadata": {},
   "source": [
    "#### 코드 6-7 IMDB 데이터에 Embedding 층과 분류기 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c52ec049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 300us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 5s 237us/step - loss: 0.5657 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 5s 227us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 5s 241us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 5s 240us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 225us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 220us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 213us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 212us/step - loss: 0.3022 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 5s 231us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))  # 나중에 임베딩된 입력을 Flatten 층에서 펼치기 위해 Embedding 층에 input_length를 지정합니다. Embedding 층의 출력 크기는 (samples, maxlen, 8)이 됩니다.\n",
    "model.add(Flatten())  # 3D 임베딩 텐서를 (samples, maxlen * 8) 크기의 2D 텐서로 펼칩니다.\n",
    "model.add(Dense(1, activation='sigmoid'))  # 분류기를 추가합니다.\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                   epochs=10,\n",
    "                   batch_size=32,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f70a5",
   "metadata": {},
   "source": [
    "약 75% 정도의 검증 정확도가 나옵니다. 리뷰에서 20개의 단어만 사용한 것치고 꽤 좋은 결과입니다. 하지만 임베딩 시퀀스를 펼치고 하나의 Dense 층을 훈련했으므로 입력 시퀀스에 있는 각 단어를 독립적으로 다루었습니다. 단어 사이의 관계나 문장 구조를 고려하지 않았습니다(예를 들어 이 모델은 \"this movie is a bomb\"과 \"this movie is the bomb\"을 부정적인 리뷰로 동일하게 다룰 것입니다). 각 시퀀스 전체를 고려한 특성을 학습하도록 임베딩 층 위에 순환 층이나 1D 합성곱 층을 추가하는 것이 좋습니다.\n",
    "\n",
    "#### 사전 훈련된 단어 임베딩 사용하기\n",
    "풀려는 문제와 함께 단어 임베딩을 학습하는 대신에 미리 계산된 임베딩 공간에서 임베딩 벡터를 로드할 수 있습니다. 자연어 처리에서 사전 훈련된 단어 임베딩을 사용하는 이유는 이미지 분류 문제에서 사전 훈련된 컨브넷을 사용하는 이유와 거의 동일합니다. 충분한 데이터가 없어서 자신만의 좋은 특성을 학습하지 못하지만 꽤 일반적인 특성이 필요할 때입니다.\n",
    "\n",
    "단어 임베딩은 일반적으로 (문장이나 문서에 같이 등장하는 단어를 관찰하는) 단어 출현 통계를 사용하여 계산됩니다.\n",
    "\n",
    "### 6.1.3 모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지\n",
    "\n",
    "#### 원본 IMDB 텍스트 내려 받기\n",
    "\n",
    "#### 코드 6-8 IMDB 원본 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aee4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = './datasets/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd6e22",
   "metadata": {},
   "source": [
    "#### 데이터 토큰화\n",
    "\n",
    "#### 코드 6-9 IMDB 원본 데이터의 텍스트를 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3599cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88582개의 고유한 토큰을 찾았습니다.\n",
      "데이터 텐서의 크기: (25000, 100)\n",
      "레이블 텐서의 크기: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100  # 100개 단어 이후는 버립니다.\n",
    "training_samples = 200  # 훈련 샘플은 200개입니다.\n",
    "validation_samples = 10000  # 검증 샘플은 1만 개입니다.\n",
    "max_words = 10000  # 데이터셋에서 가장 빈도 높은 1만 개의 단어만 사용합니다.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('%s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('데이터 텐서의 크기:', data.shape)\n",
    "print('레이블 텐서의 크기:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])  # 데이터를 훈련 세트와 검증 세트로 분할합니다. 샘플이 순서대로 있기 때문에 (부정 샘플이 모두 나온 후 긍정 샘플이 옵니다) 먼저 데이터를 섞습니ㅡ다.\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e6965c",
   "metadata": {},
   "source": [
    "#### GloVe 단어 임베딩 내려받기(다운로드 안되어 생략)\n",
    "\n",
    "### 6.1.4 정리\n",
    "- 원본 텍스트를 신경망이 처리할 수 있는 형태로 변환합니다.\n",
    "- 케라스 모델에 Embedding 층을 추가하여 어떤 작업에 특화된 토큰 임베딩을 학습하니다.\n",
    "- 데이터가 부족한 자연어 처리 문제에서 사전 훈련된 단어 임베딩을 사용하여 성능 향상을 꾀합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
